{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6726f6af-fc45-4b90-a63b-3c081c8522fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://192.168.1.178:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://192.168.1.178:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import gradio as gr  # 用于创建 Web 界面\n",
    "import os  # 用于与操作系统交互，如读取环境变量\n",
    "\n",
    "# 定义一个函数来根据输入生成文本\n",
    "def generate(input, temperature):\n",
    "    \"\"\"\n",
    "    该函数用于根据输入生成文本。\n",
    "\n",
    "    参数:\n",
    "    input: 输入内容。\n",
    "    temperature: LLM 的温度系数。\n",
    "\n",
    "    返回:\n",
    "    output: 生成的文本。\n",
    "    \"\"\"\n",
    "    # 使用预定义的 llm 对象的 predict 方法，从输入生成文本\n",
    "    output = llm.predict(input, temperature=temperature)\n",
    "    return output  # 返回生成的文本\n",
    "\n",
    "# 创建一个 Web 界面\n",
    "# 输入：一个文本框和一个滑块\n",
    "# 输出：一个文本框显示生成的文本\n",
    "demo = gr.Interface(\n",
    "    fn=generate, \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),  # 文本输入框\n",
    "        gr.Slider(label=\"Temperature\", value=0,  maximum=1, minimum=0)  # 滑块用于选择模型的 temperature\n",
    "    ], \n",
    "    outputs=[gr.Textbox(label=\"Completion\")],  # 显示生成文本的文本框\n",
    "    title=\"大模型应用开发\",  # 界面标题\n",
    "    description=\"LLM-UNIVERSE\",  # 界面描述\n",
    "    # allow_flagging=\"never\", \n",
    ")\n",
    "\n",
    "# 关闭可能已经启动的任何先前的 gradio 实例\n",
    "gr.close_all()\n",
    "\n",
    "# 启动 Web 界面\n",
    "# 使用环境变量 PORT1 作为服务器的端口号\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch(server_name=\"192.168.1.178\") # 直接启动页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199e78de-6b95-46cf-bd40-33bda90c243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../QA_Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2749ed-a7a4-4554-a614-b66fb415b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载.env文件\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "_ = load_dotenv('../QA_Project/.env')\n",
    "\n",
    "# 获取环境变量 \n",
    "wenxin_api_key = os.environ[\"wenxin_api_key\"]\n",
    "wenxin_secret_key = os.environ[\"wenxin_secret_key\"]\n",
    "spark_appid = os.environ[\"spark_app_id\"]\n",
    "spark_api_secret = os.environ[\"spark_secret_key\"]\n",
    "spark_api_key = os.environ[\"spark_api_key\"]\n",
    "zhipuai_api_key = os.environ[\"ZHIPUAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e592a78-68f9-4ce4-8c7c-cdca369c4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://192.168.1.178:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://192.168.1.178:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from QA_Project.project.llm.call_llm import get_completion\n",
    "\n",
    "llm = \"ERNIE-Bot-4\" ## 定义自己想调用的 LLM 名称\n",
    "# 定义一个函数，用于格式化聊天 prompt。\n",
    "def format_chat_prompt(message, chat_history):\n",
    "    \"\"\"\n",
    "    该函数用于格式化聊天 prompt。\n",
    "\n",
    "    参数:\n",
    "    message: 当前的用户消息。\n",
    "    chat_history: 聊天历史记录。\n",
    "\n",
    "    返回:\n",
    "    prompt: 格式化后的 prompt。\n",
    "    \"\"\"\n",
    "    # 初始化一个空字符串，用于存放格式化后的聊天 prompt。\n",
    "    prompt = \"\"\n",
    "    # 遍历聊天历史记录。\n",
    "    for turn in chat_history:\n",
    "        # 从聊天记录中提取用户和机器人的消息。\n",
    "        user_message, bot_message = turn\n",
    "        # 更新 prompt，加入用户和机器人的消息。\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    # 将当前的用户消息也加入到 prompt中，并预留一个位置给机器人的回复。\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    # 返回格式化后的 prompt。\n",
    "    return prompt\n",
    "\n",
    "# 定义一个函数，用于生成机器人的回复。\n",
    "def respond(message, chat_history):\n",
    "    \"\"\"\n",
    "    该函数用于生成机器人的回复。\n",
    "\n",
    "    参数:\n",
    "    message: 当前的用户消息。\n",
    "    chat_history: 聊天历史记录。\n",
    "\n",
    "    返回:\n",
    "    \"\": 空字符串表示没有内容需要显示在界面上，可以替换为真正的机器人回复。\n",
    "    chat_history: 更新后的聊天历史记录\n",
    "    \"\"\"\n",
    "    # 调用上面的函数，将用户的消息和聊天历史记录格式化为一个 prompt。\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "    # 使用之前定义的 get_completion 方法生成机器人的回复。\n",
    "    bot_message = get_completion(\n",
    "            formatted_prompt, llm)\n",
    "    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n",
    "    chat_history.append((message, bot_message))\n",
    "    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n",
    "    return \"\", chat_history\n",
    "\n",
    "# 下面的代码是设置Gradio界面的部分。\n",
    "\n",
    "\n",
    "# 使用Gradio的Blocks功能定义一个代码块。\n",
    "with gr.Blocks() as demo:\n",
    "    # 创建一个Gradio聊天机器人组件，设置其高度为240。\n",
    "    chatbot = gr.Chatbot(height=240) \n",
    "    # 创建一个文本框组件，用于输入  prompt。\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    # 创建一个提交按钮。\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # 设置按钮的点击事件。当点击时，调用上面定义的respond函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n",
    "\n",
    "# 关闭所有已经存在的 Gradio 实例。\n",
    "gr.close_all()\n",
    "# 启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch(server_name=\"192.168.1.178\",server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f77cfa1-c3a5-4f56-8a88-6551e4e67011",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_with_his_btn = gr.Button(\"Chat db with history\")\n",
    "db_wo_his_btn = gr.Button(\"Chat db without history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32846d45-8317-497b-bac4-ac2fa66b489a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model_center():\n",
    "    \"\"\"\n",
    "    存储问答 Chain 的对象 \n",
    "\n",
    "    - chat_qa_chain_self: 以 (model, embedding) 为键存储的带历史记录的问答链。\n",
    "    - qa_chain_self: 以 (model, embedding) 为键存储的不带历史记录的问答链。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.chat_qa_chain_self = {}\n",
    "        self.qa_chain_self = {}\n",
    "\n",
    "    def chat_qa_chain_self_answer(self, question: str, chat_history: list = [],\n",
    "                                  model: str = \"ERNIE-Bot-4\", embedding: str = \"wenxin\", \n",
    "                                  temperature: float = 0.1, top_k: int = 4,\n",
    "                                  history_len: int = 3, file_path: str = DEFAULT_DB_PATH, \n",
    "                                  persist_path: str = DEFAULT_PERSIST_PATH):\n",
    "        \"\"\"\n",
    "        调用带历史记录的问答链进行回答\n",
    "        \"\"\"\n",
    "        if question == null or len(question) < 1:\n",
    "            return \"\", chat_history\n",
    "        try:\n",
    "            if (model, embedding) not in self.chat_qa_chain_self:\n",
    "                self.chat_qa_chain_self[(model, embedding)] = Chat_QA_chain_self(model=model, \n",
    "                                                                                 temperature=temperature,\n",
    "                                                                                 top_k=top_k, \n",
    "                                                                                 chat_history=chat_history, \n",
    "                                                                                 file_path=file_path, \n",
    "                                                                                 persist_path=persist_path, \n",
    "                                                                                 embedding=embedding)\n",
    "            chain = self.chat_qa_chain_self[(model, embedding)]\n",
    "            return \"\", chain.answer(question=question, temperature=temperature, top_k=top_k)\n",
    "        except Exception as e:\n",
    "            return e, chat_history\n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = [], \n",
    "                             model: str = \"ERNIE-Bot-4\", embedding=\"wenxin\", \n",
    "                             temperature: float = 0.1, top_k: int = 4, \n",
    "                             file_path: str = DEFAULT_DB_PATH, \n",
    "                             persist_path: str = DEFAULT_PERSIST_PATH):\n",
    "        \"\"\"\n",
    "        调用不带历史记录的问答链进行回答\n",
    "        \"\"\"\n",
    "        if question == null or len(question) < 1:\n",
    "            return \"\", chat_history\n",
    "        try:\n",
    "            if (model, embedding) not in self.qa_chain_self:\n",
    "                self.qa_chain_self[(model, embedding)] = QA_chain_self(model=model, \n",
    "                                                                       temperature=temperature,\n",
    "                                                                       top_k=top_k, \n",
    "                                                                       file_path=file_path, \n",
    "                                                                       persist_path=persist_path, \n",
    "                                                                       embedding=embedding)\n",
    "            chain = self.qa_chain_self[(model, embedding)]\n",
    "            chat_history.append(\n",
    "                (question, chain.answer(question, temperature, top_k)))\n",
    "            return \"\", chat_history\n",
    "        except Exception as e:\n",
    "            return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"\n",
    "        清除 Chain 的 history\n",
    "        \"\"\"\n",
    "        if len(self.chat_qa_chain_self) > 0:\n",
    "            for chain in self.chat_qa_chain_self.values():\n",
    "                chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebe6ec-d6f8-43bf-8138-883ee537a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置按钮的点击事件。\n",
    "# 当点击时，调用上面定义的 Chat_QA_chain_self 函数，\n",
    "# 并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "db_with_his_btn.click(Chat_QA_chain_self.answer, \n",
    "                      inputs=[msg, chatbot, llm, embeddings, history_len, top_k, temperature], outputs=[msg, chatbot])\n",
    "# 设置按钮的点击事件。\n",
    "# 当点击时，调用上面定义的 QA_chain_self 函数，\n",
    "# 并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "db_wo_his_btn.click(QA_chain_self.answer, \n",
    "                    inputs=[msg, chatbot, llm, embeddings, top_k, temperature], outputs=[msg, chatbot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d356b-d44d-4fea-b152-d695bac57b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.File(label='请选择知识库目录',file_count='directory',\n",
    "                file_types=['.txt', '.md', '.docx', '.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1a4c1-d5d3-44eb-b61f-3ab35b4539a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = gr.Slider(0,\n",
    "        1,\n",
    "        value=0.00,\n",
    "        step=0.01,\n",
    "        label=\"llm temperature\",\n",
    "        interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5a057-64b5-496f-aed1-e06ef694e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = gr.Dropdown(\n",
    "    LLM_MODEL_LIST,\n",
    "    label=\"large language model\",\n",
    "    value=INIT_LLM,\n",
    "    interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8384c75-daaf-4611-9225-4452d9dde703",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = gr.Accordion(\"模型选择\")\n",
    "with model_select:\n",
    "    llm = gr.Dropdown(...)\n",
    "    embedding = gr.Dropdown(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
